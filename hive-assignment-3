
1. Download vechile sales data -> https://github.com/shashank-mishra219/Hive-Class/blob/main/sales_order_data.csv

2. Store raw data into hdfs location

3. Create a internal hive table "sales_order_csv" which will store csv data sales_order_csv .. make sure to skip header row while creating table

4. Load data from hdfs path into "sales_order_csv" 

5. Create an internal hive table which will store data in ORC format "sales_order_orc"

6. Load data from "sales_order_csv" into "sales_order_orc"


Perform below menioned queries on "sales_order_orc" table :

a. Calculatye total sales per year
b. Find a product for which maximum orders were placed
c. Calculate the total sales for each quarter
d. In which quarter sales was minimum
e. In which country sales was maximum and in which country sales was minimum
f. Calculate quartelry sales for each city
h. Find a month for each year in which maximum number of quantities were sold



***********************************************solutions for above mentioned queries*****************************************************************


step-1
 Download or copy vechile sales data -> https://github.com/shashank-mishra219/Hive-Class/blob/main/sales_order_data.csv
 
 
step-2 
store that raw data into hdfs location 
command -> vim sales_order_data.csv
        -> paste that  vehicle sales data
        
step-3
Create an internal hive table "sales_order_csv" which will store csv data sales_order_csv

first create data base
command ->  create database hive_assign;
        ->  use hive_assign;
        
then create table "sales_order_csv" to store csv data "sales_order_csv"
command -> > Create table sales_order_csv
    > (order_no int,
    >  quantity_ordered int,
    >  price_each DOUBLE,
    >  order_line_no int,
    >  sales double,
    >  status string,
    >  qtr_id int,
    >  month_id int,
    >  year_id int,
    >  product_line string,
    >  msrp int,
    >  product_code string,
    >  phone string,
    >  city string,
    >  state string,
    >  postal_code string,
    >  country string,
    >  territory string,
    >  lastname string,
    >  firstname string,
    >  deal_size string)
    >  row format delimited
    >  fields terminated by ','
    >  tblproperties ("skip.header.line.count"="1");

step-4
Load data from hdfs path into "sales_order_csv"
command->  load data local inpath 'file:///home/cloudera/data/sales_order_data.csv/' into table sales_order_csv;

step-5
Create an internal hive table which will store data in ORC format "sales_order_orc
command-> Create table sales_order_orc
(order_no int,
 quantity_ordered int,
 price_each DOUBLE,
 order_line_no int,
 sales double,
 status string,
 qtr_id int,
 month_id int,
 year_id int,
 product_line string,
 msrp int,
 product_code string,
 phone string,
 city string,
 state string,
 postal_code string,
 country string,
 territory string,
 lastname string,
 firstname string,
 deal_size string)
 stored as orc;
 
step-6
Load data from "sales_order_csv" into "sales_order_orc"
command-> from sales_order_csv insert overwrite table sales_order_orc select *;



a. Calculatye total sales per year
  command->
  select year_id,sum(sales) from sales_order_orc group by year_id;
  
  >>>>>>>>>>>result<<<<<<<<<<<
  
  Query ID = cloudera_20230407043939_bbb90fd7-4ed6-49a8-9a25-0bea7f7f286d
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1680853947493_0002, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1680853947493_0002/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1680853947493_0002
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-04-07 04:39:41,518 Stage-1 map = 0%,  reduce = 0%
2023-04-07 04:39:57,495 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.92 sec
2023-04-07 04:40:21,213 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.14 sec
MapReduce Total cumulative CPU time: 4 seconds 140 msec
Ended Job = job_1680853947493_0002
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.14 sec   HDFS Read: 38189 HDFS Write: 62 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 140 msec
OK
2003    3516979.540000001
2004    4724162.599999997
2005    1791486.71
Time taken: 64.168 seconds, Fetched: 3 row(s) 

>>>>>>Answer<<<<<<

2003    3516979.540000001
2004    4724162.599999997
2005    1791486.71

b. Find a product for which maximum orders were placed
command->
select MAX(quantity_ordered) as maximum_order,product_line from sales_order_orc group by product_line; 
 
 >>>>>>>>>>>result<<<<<<<<<< 

Query ID = cloudera_20230407044949_12858a36-53a2-4502-9100-66f14f1efefe
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1680853947493_0003, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1680853947493_0003/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1680853947493_0003
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-04-07 04:49:56,489 Stage-1 map = 0%,  reduce = 0%
2023-04-07 04:50:13,331 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.95 sec
2023-04-07 04:50:31,478 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.18 sec
MapReduce Total cumulative CPU time: 4 seconds 180 msec
Ended Job = job_1680853947493_0003
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.18 sec   HDFS Read: 29334 HDFS Write: 96 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 180 msec
OK
97      Classic Cars
66      Motorcycles
85      Planes
55      Ships
51      Trains
70      Trucks and Buses
76      Vintage Cars

>>>>>>Answer<<<<<<

97      Classic Cars
66      Motorcycles
85      Planes
55      Ships
51      Trains
70      Trucks and Buses
76      Vintage Cars

c. Calculate the total sales for each quarter
command->
select qtr_id, sum(sales) as totalsales from sales_order_orc group by qtr_id order by qtr_id asc;

 >>>>>>>>>>>result<<<<<<<<<< 

Query ID = cloudera_20230407045858_c8af59e2-240c-48ba-af7f-b4d94d9733c0
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1680853947493_0004, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1680853947493_0004/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1680853947493_0004
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-04-07 04:58:50,697 Stage-1 map = 0%,  reduce = 0%
2023-04-07 04:59:11,431 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.39 sec
2023-04-07 04:59:32,943 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.22 sec
MapReduce Total cumulative CPU time: 4 seconds 220 msec
Ended Job = job_1680853947493_0004
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1680853947493_0005, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1680853947493_0005/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1680853947493_0005
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2023-04-07 04:59:51,511 Stage-2 map = 0%,  reduce = 0%
2023-04-07 05:00:05,291 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.52 sec
2023-04-07 05:00:24,515 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.57 sec
MapReduce Total cumulative CPU time: 3 seconds 570 msec
Ended Job = job_1680853947493_0005
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.22 sec   HDFS Read: 37538 HDFS Write: 200 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 3.57 sec   HDFS Read: 5120 HDFS Write: 76 SUCCESS
Total MapReduce CPU Time Spent: 7 seconds 790 msec
OK
1       2350817.7300000004
2       2048120.2999999986
3       1758910.8099999994
4       3874780.01
Time taken: 112.497 seconds, Fetched: 4 row(s)

 >>>>>>Answer<<<<<<
 
1       2350817.7300000004
2       2048120.2999999986
3       1758910.8099999994
4       3874780.01


d. In which quarter sales was minimum
  command->
 select qtr_id, sum(sales) as totalsales from sales_order_orc group by qtr_id order by totalsales asc limit 1;
 
  >>>>>>>>>>>result<<<<<<<<<< 
 Query ID = cloudera_20230407050606_53ac5ba0-928a-4222-ae33-f1765053de2e
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1680853947493_0006, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1680853947493_0006/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1680853947493_0006
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-04-07 05:06:48,045 Stage-1 map = 0%,  reduce = 0%
2023-04-07 05:07:03,617 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.04 sec
2023-04-07 05:07:25,465 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.05 sec
MapReduce Total cumulative CPU time: 4 seconds 50 msec
Ended Job = job_1680853947493_0006
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1680853947493_0007, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1680853947493_0007/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1680853947493_0007
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2023-04-07 05:07:45,203 Stage-2 map = 0%,  reduce = 0%
2023-04-07 05:08:01,475 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.69 sec
2023-04-07 05:08:23,319 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.96 sec
MapReduce Total cumulative CPU time: 3 seconds 960 msec
Ended Job = job_1680853947493_0007
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.05 sec   HDFS Read: 37539 HDFS Write: 200 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 3.96 sec   HDFS Read: 5241 HDFS Write: 21 SUCCESS
Total MapReduce CPU Time Spent: 8 seconds 10 msec
OK
3       1758910.8099999994
Time taken: 112.836 seconds, Fetched: 1 row(s)
 
  
 >>>>>>Answer<<<<<<
 
 3       1758910.8099999994 ( in third quarter)

e.
 1.In which country sales was maximum and in which country sales was minimum

command->  select prod,sum(q_ord) as total_quantity from sales_order_orc group by prod order by total_quantity;
 
 >>>>>>>>>>>result<<<<<<<<<< 
 
 Query ID = cloudera_20230407051111_516ee2d9-17fc-4a5e-a4fe-711bf8fbb5a9
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1680853947493_0008, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1680853947493_0008/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1680853947493_0008
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-04-07 05:12:05,251 Stage-1 map = 0%,  reduce = 0%
2023-04-07 05:12:20,046 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.21 sec
2023-04-07 05:12:40,841 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.07 sec
MapReduce Total cumulative CPU time: 4 seconds 70 msec
Ended Job = job_1680853947493_0008
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1680853947493_0009, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1680853947493_0009/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1680853947493_0009
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2023-04-07 05:12:59,368 Stage-2 map = 0%,  reduce = 0%
2023-04-07 05:13:14,536 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.81 sec
2023-04-07 05:13:36,427 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.9 sec
MapReduce Total cumulative CPU time: 3 seconds 900 msec
Ended Job = job_1680853947493_0009
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.07 sec   HDFS Read: 38414 HDFS Write: 716 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 3.9 sec   HDFS Read: 5781 HDFS Write: 15 SUCCESS
Total MapReduce CPU Time Spent: 7 seconds 970 msec
OK
USA     3627982.83
Time taken: 110.42 seconds, Fetched: 1 row(s)


 >>>>>>Answer<<<<<< 
 
 USA     3627982.83
 
 2.in which country sales was minimum
 
 command->  select country, sum(sales) as total_sales from sales_order_orc group by country order by total_sales asc limit 1;
 
  >>>>>>>>>>>result<<<<<<<<<< 
  
  Query ID = cloudera_20230407053232_e24a6ece-5654-4576-a50b-cca9cae2d302
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1680853947493_0010, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1680853947493_0010/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1680853947493_0010
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-04-07 05:32:47,291 Stage-1 map = 0%,  reduce = 0%
2023-04-07 05:33:03,744 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.05 sec
2023-04-07 05:33:23,919 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.15 sec
MapReduce Total cumulative CPU time: 4 seconds 150 msec
Ended Job = job_1680853947493_0010
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1680853947493_0011, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1680853947493_0011/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1680853947493_0011
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2023-04-07 05:33:44,444 Stage-2 map = 0%,  reduce = 0%
2023-04-07 05:34:00,538 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.55 sec
2023-04-07 05:34:21,209 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.69 sec
MapReduce Total cumulative CPU time: 3 seconds 690 msec
Ended Job = job_1680853947493_0011
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.15 sec   HDFS Read: 38331 HDFS Write: 716 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 3.69 sec   HDFS Read: 5781 HDFS Write: 17 SUCCESS
Total MapReduce CPU Time Spent: 7 seconds 840 msec
OK
Ireland 57756.43
Time taken: 117.162 seconds, Fetched: 1 row(s)

   >>>>>>Answer<<<<<< 

 Ireland 57756.43  

f. Calculate quartelry sales for each city
  command->select city,qtr_id, sum(sales) from sales_order_orc group by city,qtr_id;
  
  >>>>>>>>>>>result<<<<<<<<<< 
  
Query ID = cloudera_20230407053232_e24a6ece-5654-4576-a50b-cca9cae2d302
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1680853947493_0010, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1680853947493_0010/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1680853947493_0010
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-04-07 05:32:47,291 Stage-1 map = 0%,  reduce = 0%
2023-04-07 05:33:03,744 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.05 sec
2023-04-07 05:33:23,919 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.15 sec
MapReduce Total cumulative CPU time: 4 seconds 150 msec
Ended Job = job_1680853947493_0010
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1680853947493_0011, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1680853947493_0011/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1680853947493_0011
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2023-04-07 05:33:44,444 Stage-2 map = 0%,  reduce = 0%
2023-04-07 05:34:00,538 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.55 sec
2023-04-07 05:34:21,209 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.69 sec
MapReduce Total cumulative CPU time: 3 seconds 690 msec
Ended Job = job_1680853947493_0011
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.15 sec   HDFS Read: 38331 HDFS Write: 716 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 3.69 sec   HDFS Read: 5781 HDFS Write: 17 SUCCESS
Total MapReduce CPU Time Spent: 7 seconds 840 msec
OK
Ireland 57756.43
Time taken: 117.162 seconds, Fetched: 1 row(s)
hive>  select city,qtr_id, sum(sales) from sales_order_orc group by city,qtr_id;
Query ID = cloudera_20230407054040_dc726a01-dd11-4227-893a-b3db2ddf00fc
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1680853947493_0012, Tracking URL = http://quickstart.cloudera:8088/proxy/application_1680853947493_0012/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1680853947493_0012
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-04-07 05:40:20,679 Stage-1 map = 0%,  reduce = 0%
2023-04-07 05:40:35,413 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.07 sec
2023-04-07 05:40:55,065 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.46 sec
MapReduce Total cumulative CPU time: 4 seconds 460 msec
Ended Job = job_1680853947493_0012
MapReduce Jobs Launched:
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.46 sec   HDFS Read: 40466 HDFS Write: 4361 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 460 msec
OK
Aaarhus         4       100595.54999999999
Allentown       2       6166.8
Allentown       3       71930.61
Allentown       4       44040.73
Barcelona       2       4219.2
Barcelona       4       74192.66000000002
Bergamo         1       56181.32
Bergamo         4       81774.4
Bergen          3       16363.1
Bergen          4       95277.18000000001
Boras           1       31606.72
Boras           3       53941.69
Boras           4       48710.92
Boston          2       74994.24
Boston          3       15344.640000000001
Boston          4       63730.780000000006
Brickhaven      1       31474.78
Brickhaven      2       7277.35
Brickhaven      3       114974.54000000001
Brickhaven      4       11528.53
Bridgewater     2       75778.99
Bridgewater     4       26115.800000000003
Brisbane        1       16118.48
Brisbane        3       34100.03
Bruxelles       1       18800.09
Bruxelles       2       8411.95
Bruxelles       3       47760.48
Burbank         1       37850.079999999994
Burbank         4       8234.560000000001
Burlingame      1       13529.57
Burlingame      3       42031.83
Burlingame      4       65221.66999999999
Cambridge       1       21782.699999999997
Cambridge       2       14380.92
Cambridge       3       48828.72
Cambridge       4       54251.66
Charleroi       1       16628.16
Charleroi       2       1711.26
Charleroi       3       1637.2
Charleroi       4       13463.48
Chatswood       2       43971.43
Chatswood       3       69694.40000000001
Chatswood       4       37905.149999999994
Cowes           1       26906.68
Cowes           4       51334.16
Dublin          1       38784.47
Dublin          3       18971.96
Espoo           1       51373.490000000005
Espoo           2       31018.230000000003
Espoo           3       31569.429999999993
Frankfurt       1       48698.83
Frankfurt       4       36472.76
Gensve          1       50432.55
Gensve          3       67281.01000000001
Glen Waverly    2       14378.09
Glen Waverly    3       12334.82
Glen Waverly    4       37878.55
Glendale        1       3987.2
Glendale        2       20350.95
Glendale        3       7600.12
Glendale        4       34485.50000000001
Graz            1       8775.16
Graz            4       43488.73999999999
Helsinki        1       26422.82
Helsinki        3       42744.06
Helsinki        4       42083.5
Kobenhavn       1       58871.11
Kobenhavn       2       62091.880000000005
Kobenhavn       4       24078.610000000004
Koln            4       100306.58
Las Vegas       2       33847.619999999995
Las Vegas       3       34453.85
Las Vegas       4       14449.61
Lille           1       20178.129999999997
Lille           4       48874.280000000006
Liverpool       2       91211.05999999998
Liverpool       4       26797.210000000003
London          1       8477.220000000001
London          2       32376.289999999997
London          4       83970.03
Los Angeles     1       23889.32
Los Angeles     4       24159.14
Lule            1       9749.0
Lule            4       66005.88
Lyon            1       101339.14000000001
Lyon            4       41535.10999999999
Madrid          1       357668.48999999993
Madrid          2       339588.0500000001
Madrid          3       69714.09
Madrid          4       315580.8100000001
Makati City     1       55245.020000000004
Makati City     4       38770.71000000001
Manchester      1       51017.91999999999
Manchester      4       106789.89
Marseille       1       2317.44
Marseille       2       52481.840000000004
Marseille       4       20136.859999999997
Melbourne       1       49637.57
Melbourne       2       60135.840000000004
Melbourne       4       91222.00000000001
Minato-ku       1       38191.39
Minato-ku       2       26482.700000000004
Minato-ku       4       55888.65000000001
Montreal        2       58257.50000000001
Montreal        4       15947.29
Munich          3       34993.92
NYC             1       32647.81
NYC             2       165100.34
NYC             3       63027.919999999984
NYC             4       300011.6999999999
Nantes          1       59617.4
Nantes          2       60344.98999999999
Nantes          3       61310.88000000002
Nantes          4       23031.589999999997
Nashua          1       12133.25
Nashua          4       119552.05000000002
New Bedford     1       48578.96
New Bedford     3       45738.39
New Bedford     4       113557.50999999997
New Haven       2       36973.310000000005
New Haven       4       42498.76
Newark          1       8722.12
Newark          2       74506.06999999998
North Sydney    1       65012.42
North Sydney    3       47191.76
North Sydney    4       41791.95
Osaka           1       50490.64
Osaka           2       17114.43
Oslo            3       34145.47
Oslo            4       45078.759999999995
Oulu            1       49055.4
Oulu            2       17813.4
Oulu            3       37501.58
Paris           1       71494.18
Paris           2       80215.41999999998
Paris           3       27798.480000000003
Paris           4       89436.59999999999
Pasadena        1       44273.35999999999
Pasadena        3       55776.11999999998
Pasadena        4       4512.48
Philadelphia    1       27398.82
Philadelphia    2       7287.24
Philadelphia    4       116503.06999999999
Reggio Emilia   2       41509.94
Reggio Emilia   3       56421.65000000001
Reggio Emilia   4       44669.740000000005
Reims           1       52029.07000000001
Reims           2       18971.96
Reims           3       15146.319999999998
Reims           4       48895.59
Salzburg        2       98104.23999999999
Salzburg        3       6693.280000000001
Salzburg        4       45001.11000000001
San Diego       1       87489.22999999998
San Francisco   1       72899.19999999998
San Francisco   4       151459.48
San Jose        2       160010.26999999996
San Rafael      1       267315.26
San Rafael      2       7261.75
San Rafael      3       216297.4
San Rafael      4       163983.65
Sevilla         4       54723.62
Singapore       1       28395.19
Singapore       2       92033.77000000002
Singapore       3       90250.08
Singapore       4       77809.37000000001
South Brisbane  1       21730.03
South Brisbane  3       10640.29
South Brisbane  4       27098.800000000003
Stavern         1       54701.99999999999
Stavern         4       61897.19
Strasbourg      2       80438.48
Torino          3       94117.26000000002
Toulouse        1       15139.119999999999
Toulouse        3       17251.08
Toulouse        4       38098.240000000005
Tsawassen       2       31302.5
Tsawassen       3       43332.350000000006
Vancouver       4       75238.92
Versailles      1       5759.42
Versailles      4       59074.9
White Plains    4       85555.98999999998
Time taken: 54.08 seconds, Fetched: 182 row(s)
  
  >>>>>>Answer<<<<<<
  
  Aaarhus       4       100595.54999999999
Allentown       2       6166.8
Allentown       3       71930.61
Allentown       4       44040.73
Barcelona       2       4219.2
Barcelona       4       74192.66000000002
Bergamo         1       56181.32
Bergamo         4       81774.4
Bergen          3       16363.1
Bergen          4       95277.18000000001
Boras           1       31606.72
Boras           3       53941.69
Boras           4       48710.92
Boston          2       74994.24
Boston          3       15344.640000000001
Boston          4       63730.780000000006
Brickhaven      1       31474.78
Brickhaven      2       7277.35
Brickhaven      3       114974.54000000001
Brickhaven      4       11528.53
Bridgewater     2       75778.99
Bridgewater     4       26115.800000000003
Brisbane        1       16118.48
Brisbane        3       34100.03
Bruxelles       1       18800.09
Bruxelles       2       8411.95
Bruxelles       3       47760.48
Burbank         1       37850.079999999994
Burbank         4       8234.560000000001
Burlingame      1       13529.57
Burlingame      3       42031.83
Burlingame      4       65221.66999999999
Cambridge       1       21782.699999999997
Cambridge       2       14380.92
Cambridge       3       48828.72
Cambridge       4       54251.66
Charleroi       1       16628.16
Charleroi       2       1711.26
Charleroi       3       1637.2
Charleroi       4       13463.48
Chatswood       2       43971.43
Chatswood       3       69694.40000000001
Chatswood       4       37905.149999999994
Cowes           1       26906.68
Cowes           4       51334.16
Dublin          1       38784.47
Dublin          3       18971.96
Espoo           1       51373.490000000005
Espoo           2       31018.230000000003
Espoo           3       31569.429999999993
Frankfurt       1       48698.83
Frankfurt       4       36472.76
Gensve          1       50432.55
Gensve          3       67281.01000000001
Glen Waverly    2       14378.09
Glen Waverly    3       12334.82
Glen Waverly    4       37878.55
Glendale        1       3987.2
Glendale        2       20350.95
Glendale        3       7600.12
Glendale        4       34485.50000000001
Graz            1       8775.16
Graz            4       43488.73999999999
Helsinki        1       26422.82
Helsinki        3       42744.06
Helsinki        4       42083.5
Kobenhavn       1       58871.11
Kobenhavn       2       62091.880000000005
Kobenhavn       4       24078.610000000004
Koln            4       100306.58
Las Vegas       2       33847.619999999995
Las Vegas       3       34453.85
Las Vegas       4       14449.61
Lille           1       20178.129999999997
Lille           4       48874.280000000006
Liverpool       2       91211.05999999998
Liverpool       4       26797.210000000003
London          1       8477.220000000001
London          2       32376.289999999997
London          4       83970.03
Los Angeles     1       23889.32
Los Angeles     4       24159.14
Lule            1       9749.0
Lule            4       66005.88
Lyon            1       101339.14000000001
Lyon            4       41535.10999999999
Madrid          1       357668.48999999993
Madrid          2       339588.0500000001
Madrid          3       69714.09
Madrid          4       315580.8100000001
Makati City     1       55245.020000000004
Makati City     4       38770.71000000001
Manchester      1       51017.91999999999
Manchester      4       106789.89
Marseille       1       2317.44
Marseille       2       52481.840000000004
Marseille       4       20136.859999999997
Melbourne       1       49637.57
Melbourne       2       60135.840000000004
Melbourne       4       91222.00000000001
Minato-ku       1       38191.39
Minato-ku       2       26482.700000000004
Minato-ku       4       55888.65000000001
Montreal        2       58257.50000000001
Montreal        4       15947.29
Munich          3       34993.92
NYC             1       32647.81
NYC             2       165100.34
NYC             3       63027.919999999984
NYC             4       300011.6999999999
Nantes          1       59617.4
Nantes          2       60344.98999999999
Nantes          3       61310.88000000002
Nantes          4       23031.589999999997
Nashua          1       12133.25
Nashua          4       119552.05000000002
New Bedford     1       48578.96
New Bedford     3       45738.39
New Bedford     4       113557.50999999997
New Haven       2       36973.310000000005
New Haven       4       42498.76
Newark          1       8722.12
Newark          2       74506.06999999998
North Sydney    1       65012.42
North Sydney    3       47191.76
North Sydney    4       41791.95
Osaka           1       50490.64
Osaka           2       17114.43
Oslo            3       34145.47
Oslo            4       45078.759999999995
Oulu            1       49055.4
Oulu            2       17813.4
Oulu            3       37501.58
Paris           1       71494.18
Paris           2       80215.41999999998
Paris           3       27798.480000000003
Paris           4       89436.59999999999
Pasadena        1       44273.35999999999
Pasadena        3       55776.11999999998
Pasadena        4       4512.48
Philadelphia    1       27398.82
Philadelphia    2       7287.24
Philadelphia    4       116503.06999999999
Reggio Emilia   2       41509.94
Reggio Emilia   3       56421.65000000001
Reggio Emilia   4       44669.740000000005
Reims           1       52029.07000000001
Reims           2       18971.96
Reims           3       15146.319999999998
Reims           4       48895.59
Salzburg        2       98104.23999999999
Salzburg        3       6693.280000000001
Salzburg        4       45001.11000000001
San Diego       1       87489.22999999998
San Francisco   1       72899.19999999998
San Francisco   4       151459.48
San Jose        2       160010.26999999996
San Rafael      1       267315.26
San Rafael      2       7261.75
San Rafael      3       216297.4
San Rafael      4       163983.65
Sevilla         4       54723.62
Singapore       1       28395.19
Singapore       2       92033.77000000002
Singapore       3       90250.08
Singapore       4       77809.37000000001
South Brisbane  1       21730.03
South Brisbane  3       10640.29
South Brisbane  4       27098.800000000003
Stavern         1       54701.99999999999
Stavern         4       61897.19
Strasbourg      2       80438.48
Torino          3       94117.26000000002
Toulouse        1       15139.119999999999
Toulouse        3       17251.08
Toulouse        4       38098.240000000005
Tsawassen       2       31302.5
Tsawassen       3       43332.350000000006
Vancouver       4       75238.92
Versailles      1       5759.42
Versailles      4       59074.9
White Plains    4       85555.98999999998
  
h. Find a month for each year in which maximum number of quantities were sold
 command->select MAX(ordered_quantity) as max,month_id,year_id from sales_order_orc group by year_id;
  >>>>>>>>>>>result<<<<<<<<<< 
  
  >>>>>>Answer<<<<<<
  
  
